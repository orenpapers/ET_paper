{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "a\n",
    "#https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# !pip install multimodal_transformers --user\n",
    "# !pip uninstall -y transformers --user\n",
    "# !pip install transformers==3.1.0 --user\n",
    "# import transformers\n",
    "# transformers.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import joblib\n",
    "\n",
    "scale_col = \"x_gaze_location_standard_scaled\"\n",
    "\n",
    "def apply_phq_cutoff(df , neg_phq_cutoff, pos_phq_cutoff):\n",
    "    df[\"phq_binary_label\"] = [0.0 if x <= neg_phq_cutoff else 1.0 if x >= pos_phq_cutoff else \"other\" for x in df.phq_score]\n",
    "    df = df[df.phq_binary_label!= 'other']\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_timecols_df_for_DL(fn =\"/Users/orenkobo/Desktop/PhD_new/repos/HebLingStudy/notebooks/df.csv\",\n",
    "                           scale_col = scale_col,\n",
    "                           word_embedding_dict_fn = \"/Users/orenkobo/Desktop/PhD/HebLingStudy/Output_2021_Aug/word2alephbert_encodingdict.jbl\"):\n",
    "\n",
    "    import string, re\n",
    "    print(f\"{datetime.datetime.now()} Reading csv from {fn}\")\n",
    "    df = pd.read_csv(fn,\n",
    "                     index_col=None,\n",
    "                     converters={#'alephbert_enc': eval,\n",
    "                         scale_col : eval,\n",
    "                         # 'x_gaze_location_minmax_scaled' : eval,\n",
    "                         # 'x_gaze_location_standard_scaled' : eval,\n",
    "                         # 'target_word_x_range' : eval\n",
    "                         # 'phq_label': bool\n",
    "                     })\n",
    "    print(df.shape)\n",
    "\n",
    "    def exclude_char(s):\n",
    "        exclude = set(string.punctuation)\n",
    "        exclude.remove(\"-\")\n",
    "        return ''.join(ch for ch in s if ch not in exclude)\n",
    "\n",
    "    words_embedding_dict = joblib.load(\"word2alephbert_encodingdict.jbl\")\n",
    "    df = df[df.Sentence_type != 'F'].reset_index(drop=True)\n",
    "    df[\"words_order\"] = df[\"words_order\"].apply(lambda x : [exclude_char(w) for w in eval(x)])\n",
    "    df['word_embeddings_order'] = df[\"words_order\"].apply(lambda x : [words_embedding_dict[w] for w in x])\n",
    "\n",
    "    id_cols = [\"phq_score\",\"phq_group\",\"Subject\", \"Sentence_type\",\n",
    "               \"sentence_pupil_diameter_mean\",\"set_num\", \"words_order\",\"word_embeddings_order\"]\n",
    "    # vec_size = 3500\n",
    "    # new_colname = f\"x_gaze_location_{vec_size}\"\n",
    "    cols = [f\"timepoint#{i}\" for i in range(875)]\n",
    "    # df[new_colname] = df[\"x_gaze_location_standard_scaled\"].apply(lambda x : x[:vec_size])\n",
    "    timeseries_df = pd.DataFrame(data = df[scale_col].to_list() , columns = cols)\n",
    "    timeseries_df[id_cols] = df[id_cols]\n",
    "    timeseries_df = timeseries_df.iloc[:,200:]\n",
    "    cols = [x for x in timeseries_df.columns if \"timepoint\" in x]\n",
    "    return timeseries_df, cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 14:30:23.024512 Reading csv from /export/home/orenkobo/Aim1/paper_analysis/Artifacts/df_new_full__unsegmented_alldata_new_FINAL.csv\n",
      "(9696, 31)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2alephbert_encodingdict.jbl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-d9f7a43a61cd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0met_scale_col\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"x_gaze_location\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0moverride_cutoff\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimepoint_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_timecols_df_for_DL\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0mdf_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_col\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0met_scale_col\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m df = apply_phq_cutoff(df,\n\u001B[1;32m     10\u001B[0m                       \u001B[0mneg_phq_cutoff\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moverride_cutoff\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-4ee64b7c4a84>\u001B[0m in \u001B[0;36mget_timecols_df_for_DL\u001B[0;34m(fn, scale_col, word_embedding_dict_fn)\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mch\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mch\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mexclude\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m     \u001B[0mwords_embedding_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjoblib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"word2alephbert_encodingdict.jbl\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSentence_type\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m'F'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"words_order\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"words_order\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mexclude_char\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[0;32min\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/share/apps/anaconda3-2021.05/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    575\u001B[0m             \u001B[0mobj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_unpickle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    576\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 577\u001B[0;31m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    578\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0m_read_fileobject\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmmap_mode\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfobj\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    579\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'word2alephbert_encodingdict.jbl'"
     ]
    }
   ],
   "source": [
    "# from OutputsAnalyser.run_timepoints_utils import get_timecols_df_for_DL, apply_phq_cutoff\n",
    "import pandas as pd\n",
    "d = {}\n",
    "df_fn = \"/export/home/orenkobo/Aim1/paper_analysis/Artifacts/df_new_full__unsegmented_alldata_new_FINAL.csv\"\n",
    "# df_fn = \"/Users/orenkobo/Desktop/PhD/HebLingStudy/ts_data/Artifacts2/df_new_full__unsegmented_alldata_new_FINAL.csv\"\n",
    "et_scale_col = \"x_gaze_location\"\n",
    "override_cutoff = [7,8]\n",
    "df, timepoint_cols = get_timecols_df_for_DL(fn =df_fn, scale_col = et_scale_col)\n",
    "df = apply_phq_cutoff(df,\n",
    "                      neg_phq_cutoff = override_cutoff[0],\n",
    "                      pos_phq_cutoff = override_cutoff[1])\n",
    "\n",
    "\n",
    "cond_df = pd.get_dummies(df['Sentence_type'])\n",
    "cond_cols = cond_df.columns.tolist()\n",
    "df = pd.concat([df, cond_df],axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import AutoModelWithTabular\n",
    "\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['COMET_MODE'] = 'DISABLED'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_df = df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = np.split(data_df.sample(frac=1), [int(.8*len(data_df)), int(.9 * len(data_df))])\n",
    "print('Num examples train-val-test')\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "train_df.to_csv('train.csv')\n",
    "val_df.to_csv('val.csv')\n",
    "test_df.to_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to how we combine tabular features\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    data_path: str = field(metadata={\n",
    "        'help': 'the path to the csv file containing the dataset'\n",
    "    })\n",
    "    column_info_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n",
    "        })\n",
    "\n",
    "    column_info: dict = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            'help': 'a dict referencing the text, categorical, numerical, and label columns'\n",
    "                    'its keys are text_cols, num_cols, cat_cols, and label_col'\n",
    "        })\n",
    "\n",
    "    categorical_encode_type: str = field(default='ohe',\n",
    "                                         metadata={\n",
    "                                             'help': 'sklearn encoder to use for categorical data',\n",
    "                                             'choices': ['ohe', 'binary', 'label', 'none']\n",
    "                                         })\n",
    "    numerical_transformer_method: str = field(default='yeo_johnson',\n",
    "                                              metadata={\n",
    "                                                  'help': 'sklearn numerical transformer to preprocess numerical data',\n",
    "                                                  'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n",
    "                                              })\n",
    "    task: str = field(default=\"classification\",\n",
    "                      metadata={\n",
    "                          \"help\": \"The downstream training task\",\n",
    "                          \"choices\": [\"classification\", \"regression\"]\n",
    "                      })\n",
    "\n",
    "    mlp_division: int = field(default=4,\n",
    "                              metadata={\n",
    "                                  'help': 'the ratio of the number of '\n",
    "                                          'hidden dims in a current layer to the next MLP layer'\n",
    "                              })\n",
    "    combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n",
    "                                     metadata={\n",
    "                                         'help': 'method to combine categorical and numerical features, '\n",
    "                                                 'see README for all the method'\n",
    "                                     })\n",
    "    mlp_dropout: float = field(default=0.1,\n",
    "                               metadata={\n",
    "                                   'help': 'dropout ratio used for MLP layers'\n",
    "                               })\n",
    "    numerical_bn: bool = field(default=True,\n",
    "                               metadata={\n",
    "                                   'help': 'whether to use batchnorm on numerical features'\n",
    "                               })\n",
    "    use_simple_classifier: str = field(default=True,\n",
    "                                       metadata={\n",
    "                                           'help': 'whether to use single layer or MLP as final classifier'\n",
    "                                       })\n",
    "    mlp_act: str = field(default='relu',\n",
    "                         metadata={\n",
    "                             'help': 'the activation function to use for finetuning layers',\n",
    "                             'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n",
    "                         })\n",
    "    gating_beta: float = field(default=0.2,\n",
    "                               metadata={\n",
    "                                   'help': \"the beta hyperparameters used for gating tabular data \"\n",
    "                                           \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n",
    "                               })\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.column_info != self.column_info_path\n",
    "        if self.column_info is None and self.column_info_path:\n",
    "            with open(self.column_info_path, 'r') as f:\n",
    "                self.column_info = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_feats = ['A','B','C','D']\n",
    "num_feats = ['sentence_pupil_diameter_mean']\n",
    "nlp_feats = ['words_order']\n",
    "\n",
    "text_cols = nlp_feats# ['Title', 'Review Text']\n",
    "cat_cols = cat_feats# ['Clothing ID', 'Division Name', 'Department Name', 'Class Name']\n",
    "numerical_cols = num_feats#['Rating', 'Age', 'Positive Feedback Count']\n",
    "\n",
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'num_cols': numerical_cols,\n",
    "    'cat_cols': cat_cols,\n",
    "    'label_col': 'phq_binary_label',\n",
    "    'label_list': [0,1]\n",
    "}\n",
    "\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    # model_name_or_path= '/Users/orenkobo/Desktop/PhD/Aim1/LM/alephbert-base' #'bert-base-uncased'\n",
    "    model_name_or_path= \"/export/home/orenkobo/Aim1/paper_analysis/LM/alephbert-base\"\n",
    ")\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path='.',\n",
    "    combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n",
    "    column_info=column_info_dict,\n",
    "    task='classification'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./multimodal_logs/model_name\",\n",
    "    logging_dir=\"./multimodal_logs/runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    evaluate_during_training=True,\n",
    "    logging_steps=25,\n",
    "    eval_steps=250\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/export/home/orenkobo/Aim1/paper_analysis/LM/alephbert-base\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "    data_args.data_path,\n",
    "    data_args.column_info['text_cols'],\n",
    "    tokenizer,\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    label_list=data_args.column_info['label_list'],\n",
    "    categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols=data_args.column_info['num_cols'],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_labels = len(np.unique(train_dataset.labels))\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tabular_config = TabularConfig(num_labels=num_labels,\n",
    "                               cat_feat_dim=train_dataset.cat_feats.shape[1],\n",
    "                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n",
    "                               **vars(data_args))\n",
    "config.tabular_config = tabular_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelWithTabular.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir\n",
    ")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import auc,  precision_recall_curve, roc_auc_score, accuracy_score, f1_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "def calc_classification_metrics(p: EvalPrediction):\n",
    "    pred_labels = np.argmax(p.predictions, axis=1)\n",
    "    pred_scores = softmax(p.predictions, axis=1)[:, 1]\n",
    "    labels = p.label_ids\n",
    "\n",
    "    roc_auc_pred_score = roc_auc_score(labels, pred_scores)\n",
    "    acc_score = accuracy_score(labels, pred_labels)\n",
    "    precisions, recalls, thresholds = precision_recall_curve(labels,\n",
    "                                                             pred_scores)\n",
    "    fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
    "    fscore[np.isnan(fscore)] = 0\n",
    "    ix = np.argmax(fscore)\n",
    "    threshold = thresholds[ix].item()\n",
    "    pr_auc = auc(recalls, precisions)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, pred_labels, labels=[0, 1]).ravel()\n",
    "    result = {'roc_auc': roc_auc_pred_score,\n",
    "              'threshold': threshold,\n",
    "              'pr_auc': pr_auc,\n",
    "              'recall': recalls[ix].item(),\n",
    "              'accuracy' : acc_score,\n",
    "              'precision': precisions[ix].item(), 'f1': fscore[ix].item(),\n",
    "              'tn': tn.item(), 'tp': tp.item(), 'fp': fp.item(), 'fn': fn.item()\n",
    "              }\n",
    "\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 3\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=calc_classification_metrics,\n",
    ")\n",
    "trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "i = 0\n",
    "if training_args.do_eval:\n",
    "    print(\"*** Evaluate ***\")\n",
    "    eval_result = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    print(pformat(eval_result, indent=4))\n",
    "    output_eval_file = os.path.join(\n",
    "        training_args.output_dir, f\"eval_metric_results_fold_{i+1}.txt\"\n",
    "    )\n",
    "\n",
    "    # with open(output_eval_file, \"w\") as writer:\n",
    "    #     print(\"***** Eval results *****\")\n",
    "    #     for key, value in eval_result.items():\n",
    "    #         print(\"  %s = %s\", key, value)\n",
    "    #         writer.write(\"%s = %s\\n\" % (key, value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_probs = trainer.predict(test_dataset=test_dataset).predictions\n",
    "predictions = np.argmax(predictions_probs, axis=1)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = (true_values == predictions).sum() / N\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_fold():\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-93a4d56fdffd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     _, logits, classifier_outputs = model(\n\u001B[0;32m----> 5\u001B[0;31m         \u001B[0mmodel_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0mtoken_type_ids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'token_type_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0mcat_feats\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcategorical_feat\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, logits, classifier_outputs = model(\n",
    "        model_inputs['input_ids'],\n",
    "        token_type_ids=model_inputs['token_type_ids'],\n",
    "        cat_feats=categorical_feat,\n",
    "        numerical_feats=numerical_feat\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = 2\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import BertConfig\n",
    "# import torch\n",
    "# from multimodal_transformers.model import BertWithTabular\n",
    "# from multimodal_transformers.model import TabularConfig\n",
    "# from transformers import BertTokenizer\n",
    "# from sklearn.model_selection import LeavePGroupsOut\n",
    "#\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"/Users/orenkobo/Desktop/PhD/Aim1/LM/alephbert-base\")\n",
    "# bert_config = BertConfig.from_pretrained('/Users/orenkobo/Desktop/PhD/Aim1/LM/alephbert-base')\n",
    "#\n",
    "\n",
    "# cat_feats = ['A','B','C','D']\n",
    "# num_feats = ['sentence_pupil_diameter_mean']\n",
    "# nlp_feat = 'words_order'\n",
    "#\n",
    "# tabular_config = TabularConfig(\n",
    "#     combine_feat_method='attention_on_cat_and_numerical_feats',  # https://multimodal-toolkit.readthedocs.io/en/latest/notes/combine_methods.html combining tabular data\n",
    "#     cat_feat_dim=len(cat_feats),  # need to specify this\n",
    "#     numerical_feat_dim=len(num_feats),  # need to specify this\n",
    "#     num_labels=2,   # need to specify this, assuming our task is binary classification\n",
    "#     use_num_bn=False,\n",
    "# )\n",
    "# 'onlplab/alephbert-base'\n",
    "\n",
    "\n",
    "# X = df[cat_feats + num_feats + [nlp_feat]]\n",
    "# y = df['phq_binary_label']\n",
    "# groups = df['Subject']\n",
    "\n",
    "\n",
    "# lpgo = LeavePGroupsOut(n_groups=10)\n",
    "# lpgo.get_n_splits(groups=groups)\n",
    "\n",
    "# i = 0\n",
    "# for train_index, test_index in lpgo.split(X = X , y = y, groups = groups):\n",
    "#     if i == 1:\n",
    "#         break\n",
    "#\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "#\n",
    "#     train_texts = [\" \".join(x) for x in X_train[nlp_feat]]\n",
    "#     train_model_inputs = tokenizer(train_texts, padding=True, padding_side='left')\n",
    "#     print(\"AAAAAAA1\")\n",
    "#     train_numerical_feat = torch.tensor(X_train[num_feats].values).float()\n",
    "#     train_categorical_feat = torch.tensor(X_train[cat_feats].values).float()\n",
    "#     print(\"AAAAAAA2\")\n",
    "#     train_model_inputs['cat_feats'] = train_categorical_feat\n",
    "#     train_model_inputs['num_feats'] = train_numerical_feat\n",
    "#     labels = torch.tensor(y_train.values)\n",
    "#     train_model_inputs['labels'] = labels\n",
    "#     print(\"AAAAAAA3\")\n",
    "#     test_texts = [\" \".join(x) for x in X_test[nlp_feat]]\n",
    "#     test_model_inputs = tokenizer(test_texts, padding=True)\n",
    "#     test_numerical_feat = torch.tensor(X_test[num_feats].values).float()\n",
    "#     test_categorical_feat = torch.tensor(X_test[cat_feats].values).float()\n",
    "#\n",
    "#     test_model_inputs['cat_feats'] = test_categorical_feat\n",
    "#     test_model_inputs['num_feats'] = test_numerical_feat\n",
    "#     print(\"AAAAAAA4\")\n",
    "#     bert_config.tabular_config = tabular_config\n",
    "#\n",
    "#     model = BertWithTabular.from_pretrained('/Users/orenkobo/Desktop/PhD/Aim1/LM/alephbert-base', config=bert_config)\n",
    "#     print(\"AAAAAAA5\")\n",
    "#     i+=1\n",
    "\n",
    "# loss, logits, layer_outs = model(\n",
    "#     torch.tensor(train_model_inputs['input_ids']),\n",
    "#     token_type_ids=torch.tensor(train_model_inputs['token_type_ids']),\n",
    "#     labels=labels,\n",
    "#     cat_feats=train_categorical_feat,\n",
    "#     numerical_feats=train_numerical_feat\n",
    "# )\n",
    "# print(\"AAAAAAA6\")\n",
    "# print(\"AAAAAAA6\")\n",
    "# model\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    _, logits, classifier_outputs = model(\n",
    "#        torch.tensor(test_model_inputs['input_ids']),\n",
    "#        token_type_ids=torch.tensor(test_model_inputs['token_type_ids']),\n",
    "#        cat_feats=test_categorical_feat,\n",
    "#        numerical_feats=test_numerical_feat\n",
    "\n",
    "#    )#%%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}